---
title: "Navigating dataaimsr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Navigating dataaimsr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The very first thing to do is read the documentation on our
[README](../index.html) page. Make sure you have the package properly 
installed, and that your personal [AIMS Data Platform API Key][1] has 
been downloaded.

[0]: https://www.aims.gov.au/
[1]: https://open-aims.github.io/data-platform/key-request

Once you have obtained your API Key, you can either place it 
permanently on your `.Renviron` file and set the object `my_api_key` to 
`NULL` in the chunk below, or if for some reason you are having 
difficulty placing your API Key permanently to the `.Renviron`, then 
just paste it to the object `my_api_key` below.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggmap)
library(dataaimsr)
my_api_key <- "1NIik2HDo36shE3FG3MfFMLlNBfEOL945FIH7fX0"
```

```{r, eval = FALSE}
# set my_api_key to NULL if successfully placed in .Renviron
# paste your key where it says api-key-for-r-notebook
my_api_key <- "api-key-for-r-notebook"
```

Then install `dataaimsr` following the documentation on our
[README](../index.html) page. Once that's out of the way, we load the packages needed for this vignette:

```{r, eval = FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggmap)
library(dataaimsr)
```

## How this package works

`dataaimsr` contains two sets of monitoring data collected by AIMS---[the
Australian Institute of Marine Science][0]---since the 1980's: the
[Weather Station][2] dataset which contains 
encompasses data for different parameters (e.g. Air Temperature, Air 
Pressure, Chlorophyll, and many others); and the
[Sea Water Temperature Loggers][3] dataset which contains records of 
(you guessed it!) sea water temperature at different sites and water 
depths.

[2]: https://doi.org/10.25845/5c09bf93f315d
[3]: https://doi.org/10.25845/5b4eb0f9bb848

The datasets are very large, and as such they are not locally stored.
They are instead downloaded via the API and unique DOI identifier (just 
hover over the data links above to see the actual DOI codes). The 
datasets are structured by sites, series and parameters. A series is a 
continuing time-series, i.e. a collection of deployments measuring the 
same parameter (e.g. Air Temperature, Air Pressure, Chlorophyll) at the 
same subsite. So, for a given site and parameter, there might exist multiple
subsites and therefore series, in which case they are most likely 
distinguishable by depth.

For the Sea Water Temperature Loggers dataset, series is synonymous 
with the variable called subsite. For the Weather Station dataset, it 
is the combination of subsite and parameter.

This vignette gives an overview of how one would go about discovering
the overall information contained in the datasets. For dataset-specific 
vignettes, see our other [vignette pages][4].

[4]: https://open-aims.github.io/dataaimsr/articles

## Discover a dataset

The [AIMS Data Platform API][5] points to the full metadata of each
dataset. We are currently working on ways to facilitate the 
visualisation of both datasets and their multiple features directly
through the R package. At the moment though it is only possible
to visualise summary information for the Sea Water Temperature Loggers
dataset. A similar feature for the Weather Station dataset will be 
implemented in the near future (likely early 2021)---so for now, please
refer to the online metadata to discover from where (and when) you can 
download data.

[5]: https://open-aims.github.io/data-platform 

### Data summary

The first step would be to visualise the dataset. Let's do this by
mapping all available sites for the Sea Water Temperature 
Loggers dataset using the main function called `aims_data`:

```{r, message = FALSE, warning = FALSE}
sdata <- aims_data(target = "temp_loggers", api_key = my_api_key,
                   summary = "summary-by-series")
head(sdata)
```

The `summary` argument here is key. It should only be flagged when the
user wants an overview of the available data. Again, this currently
implemented for the Sea Water Temperature Loggers dataset. One can
visualise `summary-by-series` or `summary-by-deployment`. The output of
`aims_data` when summary is `NA` (the default) is a `data.frame`.

Notice that `sdata` contains a lot of information, most of which is
related to site / series / parameter ID. Each row corresponds to a
unique series, and a certain site may contain multiple series; in such
cases, series generally differ from one another by depth. The columns 
`time_coverage_start` and `time_coverage_end` are probably one of the most
valuable pieces of information. They provide the user with the window of data
collection for a particular series, which is probably crucial to decide
whether that particular series is of relevance to the specific question in
hand.

Also note that there are three columns containing the total number of 
observations in a series: `uncal_obs`, `cal_obs` and `qc_obs`, which 
respectively stand for uncalibrated, calibrated, and quality-controlled 
observations. Calibrated and quality-controlled are generally the same,
and currently there is no public information on AIMS quality-control
methods and algorithms. However, if this is crucial information to you,
please reach out to the
[Data Manager from the AIMS Data Centre](adc@aims.gov.au). So let's
go ahead and plot these data on a map of Australia, while colouring
based on the total amount of calibrated observations:

```{r, fig.width = 7, fig.height = 4.36, message = FALSE, warning = FALSE}
sdata <- sdata %>%
  mutate(cols = cal_obs * 1e-3) %>%
  drop_na(lon, lat)
lab <- expression(atop("# Calibrated obs.",
                       paste("(" %*% 10^3, ")")))
ozmap <- get_stamenmap(bbox = c(left = 94, bottom = -45, right = 162, top = 0),
                       zoom = 4, maptype = "watercolor")
ggmap(ozmap) +
  theme_void() +
  geom_point(data = sdata,
             mapping = aes(x = lon, y = lat,
                           colour = cols,
                           fill = cols),
             shape = 16, alpha = 0.8) +
  scale_colour_gradient(name = lab,
                        low = "dodgerblue4",
                        high = "darkred") +
  scale_fill_gradient(name = lab,
                      low = "dodgerblue4",
                      high = "darkred")
```

### Filter values

In the case of the Weather Station dataset, knowing what sites are
out there is a bit tricky. However, currently we have a convenience
function called `filter_values` which allows one to query what
sites, series and parameters are available for both datasets:

```{r, message = FALSE, warning = FALSE}
filter_values("weather", filter_name = "series") %>%
  head()
```

The downside is that one cannot know what time window is available
for each one of those, nor how they are nested (i.e. series /
parameter / site). In a way though the series name generally
gives that information anyway (see code output above). If knowing the 
available observation window is absolutely crucial, then as mentioned 
above the user should refer to the [online metadata][5].

## Download slices of datasets

Now that we know how to explore the datasets and what data is out there,
we finish this vignette by showing an example of how one would go about
downloading actual data.

We say slices of datasets because AIMS monitoring datasets are of very 
high temporal resolution and if one tries to download the entire thing
it might take hours if not days. Generally that is why we download
slices of data at a time, and for that we need filters.

### Data filters

Filters are the last important information the user needs to know to 
master the navigation and download of AIMS monitoring datasets. Each 
dataset can filtered by attributes which can be exposed with the function `expose_attributes`:

```{r, message = FALSE, warning = FALSE}
expose_attributes("weather")
expose_attributes("temp_loggers")
```

The help file (see `?expose_attributes`) contains the details about what
each filter targets. So, having an understanding of the summaries and what
filters are available provide the user with a great head start.

Downloading the data is achieved using the same `aims_data` function, 
however now we do not specify a `summary` argument, and instead 
implement filters. For example, let's say we want to download all the
data collected at the [Yongala](https://en.wikipedia.org/wiki/SS_Yongala) for
a specific time window:

```{r, message = FALSE, warning = FALSE}
wdata_a <- aims_data("weather", api_key = my_api_key,
                     filters = list(site = "Yongala",
                                    from_date = "2018-01-01",
                                    thru_date = "2018-01-02"))
```

The output of `aims_data` when summary is `NA` (the default) is a list
containing three elements:

- `metadata` a doi link containing the metadata record for the data series

- `citation` the citation information for the particular dataset

- `data` an output `data.frame`

```{r}
wdata_a$metadata
```

```{r}
wdata_a$citation
```

```{r}
head(wdata_a$data)
```

We see that there a bunch of parameters available for this site at the
specified time:

```{r}
unique(wdata_a$data$parameter)
```

And the actual measurements are either raw or quality-controlled. Let's
plot wind speed data as an example

```{r, message = FALSE, warning = FALSE, fig.width = 7.4, fig.height = 5.2}
wdata_a$data %>%
  filter(grepl("Wind Speed", parameter)) %>%
  ggplot(data = .) +
    geom_line(aes(x = time, y = qc_val, colour = parameter)) +
    labs(x = "Date",
         y = "Wind speed (km / h)",
         colour = "Site",
         title = "AIMS Weather stations",
         subtitle = "Yongala (2018)") +
    theme_bw()
```

We could refine even further by including a time window to download the 
data:

```{r, message = FALSE, warning = FALSE}
wdata_b <- aims_data("weather",
                     api_key = my_api_key,
                     filters = list(series_id = 64,
                                    from_date = "1991-10-18T06:00:00",
                                    thru_date = "1991-10-18T12:00:00"))$data
range(wdata_b$time)
```

## More info

See our other [vignette pages][4] for further dataset-specific 
explorations.
